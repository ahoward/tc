# Quickstart: Test Suite Generator

**Date**: 2025-10-12
**Feature**: `tc new` command for test scaffolding
**Related**: [spec.md](spec.md) | [plan.md](plan.md) | [data-model.md](data-model.md)

## Overview

The test suite generator (`tc new`) creates complete, failing test scaffolding in seconds. It follows TDD principles: start with a clear failing test, then implement until it passes.

**Philosophy**: The best way to start testing is with a test that fails for the right reason - "NOT_IMPLEMENTED" - not because of missing files or structure.

## Quick Start

### Generate Your First Test

```bash
# Generate a basic test suite
tc new tests/my-feature

# Output:
# created test suite: tests/my-feature 🚁
#
# structure:
#   tests/my-feature/
#   ├── run
#   ├── README.md
#   └── data/
#       └── example-scenario/
#           ├── input.json
#           └── expected.json
#
# next steps:
#   1. Edit tests/my-feature/run to add test logic
#   2. Update data/example-scenario/*.json with real data
#   3. Run: tc run tests/my-feature
```

### See It Fail (TDD Step 1)

```bash
tc run tests/my-feature

# Output:
# ❌ tests/my-feature
#   example-scenario: FAIL
#
#   error: NOT_IMPLEMENTED
#   message: Test logic not yet implemented. Edit the 'run' script to add your test implementation.
#   next_steps:
#     1. Open the 'run' script in this directory
#     2. Replace the TODO section with your test logic
#     3. Read input.json and produce output matching expected.json
#     4. Run 'tc run tests/my-feature' to verify
```

Perfect! This is exactly what we want - a clear, actionable failure.

### Implement the Test (TDD Step 2)

Edit `tests/my-feature/run`:

```bash
#!/usr/bin/env bash
set -e

input_file="$1"

# Parse input
name=$(jq -r '.name' "$input_file")

# Implement test logic
result="Hello, $name!"

# Return JSON matching expected.json structure
jq -n --arg result "$result" '{
  "greeting": $result
}'
```

Update `tests/my-feature/data/example-scenario/input.json`:

```json
{
  "name": "World"
}
```

Update `tests/my-feature/data/example-scenario/expected.json`:

```json
{
  "greeting": "Hello, World!"
}
```

### See It Pass (TDD Step 3)

```bash
tc run tests/my-feature

# Output:
# ✅ tests/my-feature
#   example-scenario: PASS
```

🚁 You've completed the TDD cycle!

## Common Usage Patterns

### Hierarchical Organization

```bash
# Organize by feature area
tc new tests/auth/login
tc new tests/auth/logout
tc new tests/auth/password-reset

# Organize by priority
tc new tests/critical/data-loss-prevention
tc new tests/smoke/homepage-loads
```

### With Metadata (AI-Friendly)

```bash
# Add tags for discoverability
tc new tests/api/users --tags "api,crud,users"

# Set priority
tc new tests/security/xss-prevention --priority high

# Add description
tc new tests/performance/load --description "Test system under 1000 concurrent users"

# Specify dependencies
tc new tests/checkout/payment --depends "tests/auth/login tests/cart/add-item"
```

### From Examples

```bash
# List available examples
tc new --list-examples

# Output:
# Built-in templates:
#   default - Basic test suite structure
#
# Example templates (from examples/):
#   hello-world - Simple arithmetic test

# Generate from example
tc new tests/my-calc --from hello-world
```

### Force Overwrite

```bash
# Regenerate existing test (careful!)
tc new tests/existing-test --force
```

## Understanding Generated Files

### `run` Script

The executable that receives `input.json` and produces output:

```bash
#!/usr/bin/env bash
# Test runner for {test-name}
# Generated by tc new

set -e  # Exit on error

input_file="$1"  # First argument is path to input.json

# TODO: Parse input using jq
# value=$(jq -r '.field' "$input_file")

# TODO: Implement test logic
# result=$(your_command_here)

# TODO: Output JSON to stdout
# jq -n --arg result "$result" '{"field": $result}'

# Until implemented, return clear error
cat <<'EOF'
{
  "error": "NOT_IMPLEMENTED",
  "message": "Test logic not yet implemented."
}
EOF

exit 1
```

**Key points**:
- Must be executable (generator sets this automatically)
- Receives input file path as `$1`
- Must output JSON to stdout
- Must exit non-zero on failure
- Language-agnostic: use any language, just follow the contract

### `README.md`

AI-friendly metadata and documentation:

```markdown
# my-feature

**tags**: `pending`, `new`
**what**: TODO: describe test purpose
**depends**:
**priority**: medium

## description

TODO: Add detailed description of what this test validates

## scenarios

- `example-scenario` - TODO: Describe what this scenario tests

## ai notes

run this when: testing my-feature
skip this when: test logic not yet implemented
after this: TODO: List related tests to run
```

**Key points**:
- Structured metadata for AI parsing
- Clear TODO markers show what to fill in
- Integrates with `tc list`, `tc tags`, `tc explain`

### `data/example-scenario/`

Test scenarios with input/output pairs:

**`input.json`**: What you feed into the test
```json
{
  "TODO": "Replace with actual input data",
  "example_field": "example_value"
}
```

**`expected.json`**: What you expect the test to produce
```json
{
  "TODO": "Replace with expected output",
  "example_result": "expected_value"
}
```

**Key points**:
- One scenario = one subdirectory under `data/`
- Add more scenarios by creating more subdirectories
- Each scenario needs both `input.json` and `expected.json`

## AI Integration

Generated tests are immediately discoverable by AI assistants:

```bash
# List all pending tests
tc list --tags pending

# Explain what a test does
tc explain tests/my-feature

# Run all auth tests
tc run --tags auth

# Show all available tags
tc tags
```

The metadata in `README.md` helps AI understand:
- **What**: Purpose of the test
- **When**: Conditions to run it
- **Dependencies**: What must pass first
- **Priority**: Importance level

## Adding More Scenarios

```bash
# Your test structure
tests/my-feature/
├── run
├── README.md
└── data/
    ├── example-scenario/
    │   ├── input.json
    │   └── expected.json
    ├── edge-case-empty-input/      # Add this
    │   ├── input.json
    │   └── expected.json
    └── edge-case-special-chars/    # And this
        ├── input.json
        └── expected.json
```

Create new directories under `data/`, add `input.json` and `expected.json`, then run:

```bash
tc run tests/my-feature

# Output:
# ✅ tests/my-feature
#   example-scenario: PASS
#   edge-case-empty-input: PASS
#   edge-case-special-chars: PASS
```

tc automatically discovers all scenarios.

## Tips & Best Practices

### Naming Conventions

✅ **Good names**:
- `user-login`
- `api-v2-tokens`
- `payment-processing`

❌ **Bad names**:
- `UserLogin` (no uppercase)
- `user_login` (no underscores)
- `user login` (no spaces)

**Rule**: Lowercase letters, numbers, hyphens only. Must start and end with alphanumeric.

### Directory Organization

```bash
# By feature area
tests/
├── auth/
│   ├── login/
│   ├── logout/
│   └── password-reset/
├── api/
│   ├── users/
│   ├── posts/
│   └── comments/
└── ui/
    ├── homepage/
    └── navigation/
```

### Metadata Best Practices

**Tags**: Keep them simple and queryable
```bash
--tags "api,crud,users"      # Good: specific, searchable
--tags "test,thing,stuff"    # Bad: too vague
```

**Priority**: Use consistently across team
```bash
--priority critical   # Must pass before deploy
--priority high       # Important, run frequently
--priority medium     # Regular test coverage (default)
--priority low        # Nice to have, run occasionally
```

**Dependencies**: List tests that must pass first
```bash
--depends "tests/auth/login tests/data/seed"
```

### Test-First Workflow

1. **Generate**: `tc new tests/feature`
2. **Define**: Edit `input.json` and `expected.json` with real data
3. **Verify Failure**: Run test, see "NOT_IMPLEMENTED"
4. **Implement**: Edit `run` script with actual logic
5. **Verify Success**: Run test, see PASS
6. **Refine**: Add more scenarios, edge cases

## Troubleshooting

### "Directory already exists"

```bash
# Use --force to overwrite
tc new tests/my-feature --force

# Or remove and regenerate
rm -rf tests/my-feature
tc new tests/my-feature
```

### "Invalid test name"

```bash
# Check naming rules
tc new tests/My-Feature        # ❌ uppercase
tc new tests/my_feature        # ❌ underscore
tc new tests/my-feature        # ✅ correct
```

### "Permission denied"

```bash
# Check parent directory permissions
ls -la tests/

# Create parent manually if needed
mkdir -p tests/auth
tc new tests/auth/login
```

### Test Runs But Doesn't Fail

If generated test doesn't show "NOT_IMPLEMENTED" error:

```bash
# Check if run script is executable
ls -la tests/my-feature/run

# If not executable, fix it
chmod +x tests/my-feature/run
```

### Generated Metadata Not Showing in `tc list`

```bash
# Verify README.md format
cat tests/my-feature/README.md

# Look for required fields:
# **tags**: `...`
# **what**: ...
# **depends**: ...
# **priority**: ...
```

## Next Steps

- **Read**: [data-model.md](data-model.md) for internal structure details
- **Read**: [contracts/template-format.md](contracts/template-format.md) for template specification
- **Explore**: `examples/` directory for working test examples
- **Customize**: Create custom templates in `lib/templates/`

## Philosophy: Why Failing Tests First?

**Traditional approach**:
1. Write code
2. Hope it works
3. Add tests later (maybe)

**TDD approach with tc**:
1. Generate failing test (`tc new`)
2. See clear failure (NOT_IMPLEMENTED)
3. Implement until test passes
4. Refactor with confidence

The generator enforces TDD by making the failing test the starting point, not an afterthought.

🚁 **tc's motto**: "Start with a test that fails for the right reason, end with code that passes for the right reason."
